"""
ç«¯åˆ°ç«¯å®Œæ•´æµç¨‹æ¼”ç¤º
æ¼”ç¤ºä»å›¾ç‰‡è§£æ -> å‘é‡åŒ–å­˜å‚¨ -> æ£€ç´¢æŸ¥è¯¢çš„å®Œæ•´è¿‡ç¨‹
"""
import os
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('.')

from pipelines.indexing_pipeline import IndexingPipeline
from pipelines.retrieval_pipeline import RetrievalPipeline
from utils.logger import logger

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´æµç¨‹"""
    print("ğŸš€ QwenRag ç«¯åˆ°ç«¯æµç¨‹æ¼”ç¤º")
    print("=" * 50)
    
    # é…ç½®è·¯å¾„
    dataset_dir = "dataset"
    metadata_path = "demo_metadata.json"
    index_path = "demo_index.faiss"
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•
    if not Path(dataset_dir).exists():
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
        return
    
    try:
        # Step 1: æ„å»ºç´¢å¼•
        print("\nğŸ“š Step 1: æ„å»ºç´¢å¼•æµæ°´çº¿")
        print("-" * 30)
        
        indexing_pipeline = IndexingPipeline(
            metadata_save_path=metadata_path,
            batch_size=5,
            max_workers=2,
            auto_save=True
        )
        
        # æ„å»ºç´¢å¼•
        indexing_results = indexing_pipeline.build_index_from_directory(
            directory_path=dataset_dir,
            recursive=True,
            parallel=True,
            resume_from_metadata=True
        )
        
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
        print(f"   æ€»è®¡: {indexing_results['total']} ä¸ªæ–‡ä»¶")
        print(f"   æˆåŠŸ: {indexing_results['success']} ä¸ª")
        print(f"   å¤±è´¥: {indexing_results['failed']} ä¸ª")
        
        if indexing_results['success'] == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶ï¼Œé€€å‡ºæ¼”ç¤º")
            return
        
        # Step 2: æ£€ç´¢æ¼”ç¤º
        print("\nğŸ” Step 2: æ£€ç´¢æµæ°´çº¿æ¼”ç¤º")
        print("-" * 30)
        
        retrieval_pipeline = RetrievalPipeline(
            metadata_path=metadata_path,
            default_top_k=5,
            similarity_threshold=0.3
        )
        
        # æ¼”ç¤ºä¸åŒçš„æŸ¥è¯¢ç±»å‹
        demo_queries = [
            "æ¸¸æˆç”»é¢",
            "è§’è‰²",
            "é£æ™¯",
            "æˆªå›¾"
        ]
        
        print("\nğŸ¯ æ–‡æœ¬æŸ¥è¯¢æ¼”ç¤º:")
        for query in demo_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            results = retrieval_pipeline.search_by_text(query, top_k=3)
            
            if results:
                for i, result in enumerate(results[:3], 1):
                    print(f"  {i}. {Path(result['image_path']).name}")
                    print(f"     ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}")
                    print(f"     åŒ¹é…ç±»å‹: {result['match_type']}")
                    if result.get('metadata', {}).get('description'):
                        desc = result['metadata']['description'][:50] + "..."
                        print(f"     æè¿°: {desc}")
            else:
                print("  æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
        
        # æ¼”ç¤ºå›¾ç‰‡æŸ¥è¯¢
        print("\nğŸ–¼ï¸  ä»¥å›¾æœå›¾æ¼”ç¤º:")
        
        # æ‰¾ä¸€å¼ å›¾ç‰‡ä½œä¸ºæŸ¥è¯¢æ ·æœ¬
        sample_images = list(Path(dataset_dir).glob("*.jpg"))[:3]
        
        for sample_image in sample_images:
            print(f"\næŸ¥è¯¢å›¾ç‰‡: {sample_image.name}")
            results = retrieval_pipeline.search_by_image(str(sample_image), top_k=3)
            
            if results:
                for i, result in enumerate(results[:3], 1):
                    result_name = Path(result['image_path']).name
                    if result_name != sample_image.name:  # æ’é™¤è‡ªå·±
                        print(f"  {i}. {result_name}")
                        print(f"     ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}")
                        print(f"     åŒ¹é…ç±»å‹: {result['match_type']}")
            else:
                print("  æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡")
        
        # Step 3: ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š Step 3: ç³»ç»Ÿç»Ÿè®¡")
        print("-" * 30)
        
        indexing_stats = indexing_pipeline.get_statistics()
        retrieval_stats = retrieval_pipeline.get_statistics()
        
        print(f"ğŸ“š ç´¢å¼•ç»Ÿè®¡:")
        print(f"   å¤„ç†æ€»æ•°: {indexing_stats['total_processed']}")
        print(f"   æˆåŠŸæ•°é‡: {indexing_stats['success_count']}")
        print(f"   å¤±è´¥æ•°é‡: {indexing_stats['failed_count']}")
        print(f"   å…ƒæ•°æ®æ•°: {indexing_stats['metadata_count']}")
        
        print(f"\nğŸ” æ£€ç´¢ç»Ÿè®¡:")
        print(f"   å‘é‡æ•°é‡: {retrieval_stats['vector_count']}")
        print(f"   å…ƒæ•°æ®æ•°: {retrieval_stats['metadata_count']}")
        print(f"   å‘é‡ç»´åº¦: {retrieval_stats['index_dimension']}")
        print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {retrieval_stats['similarity_threshold']}")
        
        print("\nğŸ‰ ç«¯åˆ°ç«¯æµç¨‹æ¼”ç¤ºå®Œæˆ!")
        print("=" * 50)
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


if __name__ == "__main__":
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        print("   æŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        print("")
    
    main()